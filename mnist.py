# -*- coding: utf-8 -*-
"""kmaranga_hw4_q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mWVq7oauABK7RNMxpqqHSWxDscoqTTdt
"""

import numpy as np
import tensorflow
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt
import cv2

(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
train_idx_0 = np.argwhere(Y_train==0)
train_idx_1 = np.argwhere(Y_train==1)
test_idx_0 = np.argwhere(Y_test==0)
test_idx_1 = np.argwhere(Y_test==1)
x_train = []
y_train = []
x_test = []
y_test = []
for i in range(len(train_idx_0)):
  idx = train_idx_0[i][0]
  temp = cv2.resize(X_train[idx], (14,14))
  x_train.append(temp.flatten())
  y_train.append(Y_train[idx])

for i in range(len(train_idx_1)):
  idx = train_idx_1[i][0]
  temp = cv2.resize(X_train[idx], (14,14))
  x_train.append(temp.flatten())
  y_train.append(Y_train[idx])

for i in range(len(test_idx_0)):
  idx = test_idx_0[i][0]
  temp = cv2.resize(X_test[idx], (14,14))
  x_test.append(temp.flatten())
  y_test.append(Y_test[idx])

for i in range(len(test_idx_1)):
  idx = test_idx_1[i][0]
  temp = cv2.resize(X_test[idx], (14,14))
  x_test.append(temp.flatten())
  y_test.append(Y_test[idx])

x_train = np.array(x_train)
y_train = np.array(y_train)
x_test = np.array(x_test)
y_test = np.array(y_test)
idx_train=np.array(range(len(y_train)))
idx_test=np.array(range(len(y_test)))
np.random.shuffle(idx_train)
np.random.shuffle(idx_test)
x_tr = []
x_te = []
y_tr = []
y_te = []
for i in range(len(idx_train)):
  x_tr.append(x_train[idx_train[i]].flatten())
  y_tr.append(y_train[idx_train[i]])
for i in range(len(idx_test)):
  x_te.append(x_test[idx_test[i]].flatten())
  y_te.append(y_test[idx_test[i]])
x_tr = np.nan_to_num(np.array(x_tr)/np.sum(x_tr,axis=0))
x_te = np.nan_to_num(np.array(x_te)/np.sum(x_tr,axis=0))
y_tr = np.array(y_tr)
y_te = np.array(y_te)
y_train = []
y_test = []
for i in range(len(y_tr)):
  if y_tr[i] == 1:
    y_train.append(-1)
  else:
     y_train.append(1)

for i in range(len(y_te)):
  if y_te[i] == 1:
    y_test.append(-1)
  else:
     y_test.append(1)
y_tr = np.array(y_train)
y_te = np.array(y_test)

def grad_h(w,x,y,lambda_reg,bias):
  n = np.shape(x)[0]
  grad = -(1/n)*np.matmul(x.T,((y*(np.exp(-y*(np.matmul(w,x.T)+bias)))).T) /(np.exp(-y*(np.matmul(w,x.T)+bias))+1).T)+lambda_reg*w.T
  return grad.T

def train_gd(x_tr, y_tr, lam, lr, n_iter, w0=0,w_0 = None):
  c = 2
  d = np.shape(x_tr)[1]
  w = w_0 or np.random.rand(c,d)
  w_all = []
  t = 0
  while t <= n_iter:
    w = w - lr*grad_h(w,x_tr,y_tr,lam,w0)
    w_all.append(w)
    t += 1
  return np.array(w_all)

def loss(w, x, y, lam, bias=0):
  l1 = np.sum(np.log(1 + np.exp(-y*(np.matmul(w,x.T)+bias))+1),axis=1)/np.shape(y_tr)[0]
  l2 = (lam/2)*((np.linalg.norm(w)*np.linalg.norm(w)) + (np.linalg.norm(bias)*np.linalg.norm(bias)))
  return l1 + l2

def logit(w, x, bias=0):
  y_0 = 1
  y_1 = -1
  p = lambda x, y, w, w0: (np.exp(-y*(np.matmul(w,x.T)+w0))+1)**(-1)
  prob = np.array([p(x,y_0,w,bias),p(x,y_1,w,bias)])
  index = np.argmax(prob)
  return index

lambdas = [1] ##regularization parameter
lr = 0.01 ##learning rate
T_max = 50  #max iterations
w_all_1 = train_gd(x_tr, y_tr, lambdas[0], lr, T_max)

loss_tr_1 = [loss(w,x_tr, y_tr, lambdas[0])[0] for w in w_all_1]

t_rng = np.array(range(len(w_all_1)))
plt.semilogy(t_rng, loss_tr_1)

plt.ylabel('training loss in log-scale')
plt.xlabel('num iterations')
plt.legend(('ld =1'))
plt.show()

#get the slope of the line and confirm that it's the inverse of the condition number
slope_1 = z = (np.polyfit(t_rng[0:20], np.log(loss_tr_1[0:20]), 1))

print("slope when lambda = 1:  = " + str(slope_1[0]))
'''
we can notice that after lambda = 50, the bias term dominates due to regularisation.
the optimal lambda in our case is lambda = 10 and slope ~ -0.2.
'''

#c) write down the Hessian

#train the model on GD with Nesterov's acceleration
def train_nesterov(x_tr, y_tr, rho, lambd, lr, num_iter, bias=0,w_0 = None):
  w_all = []
  t = 0
  c = 2
  d = np.shape(x_tr)[1]
  u =  w_0 or np.random.rand(c,d)
  w =  w_0 or np.random.rand(c,d)
  while t <= num_iter:
    u_next = w - lr * grad_h(w, x_tr, y_tr, lambd, bias)
    w = (1 + rho) * u_next - rho * u
    u = u_next
    w_all.append(w)
    t += 1
  return np.array(w_all)

#each rho corresponds to a certain condition number
#use rho with increments of 0.05 between 0.75 and 1 not inclusive
rho = [0.75, 0.8, 0.85, 0.9, 0.95]
lam = 10
lr = 0.01
max_iter = 100
w_all_1 = train_nesterov(x_tr, y_tr,rho[0],lam, lr, max_iter)
'''
w_all_2 = train_nesterov(x_tr, y_tr,rho[1],lam, lr, max_iter)
w_all_3 = train_nesterov(x_tr, y_tr,rho[2],lam, lr, max_iter)
w_all_4 = train_nesterov(x_tr, y_tr,rho[3],lam, lr, max_iter)
w_all_5 = train_nesterov(x_tr, y_tr,rho[4],lam, lr, max_iter)
'''
loss_tr_1 = [loss(w,x_tr, y_tr, lam)[0] for w in w_all_1]

'''
loss_tr_2 = [loss(w,x_tr, y_tr, lam)[0] for w in w_all_2]
loss_tr_3 = [loss(w,x_tr, y_tr, lam)[0] for w in w_all_3]
loss_tr_4 = [loss(w,x_tr, y_tr, lam)[0] for w in w_all_4]
loss_tr_5 = [loss(w,x_tr, y_tr, lam)[0] for w in w_all_5]
'''

#Plot results again on a log-scale
t_rng = np.array(range(len( loss_tr_1)))
plt.semilogy(t_rng, loss_tr_1)
'''
plt.semilogy(t_rng, loss_tr_2)
plt.semilogy(t_rng, loss_tr_3)
plt.semilogy(t_rng, loss_tr_4)
plt.semilogy(t_rng, loss_tr_5)
'''
plt.legend(('rho = 0.75', 'rho = 0.80', 'rho = 0.85', 'rho = 0.90', 'rho = 0.95'))
plt.ylabel('Training loss in log-scale')
plt.xlabel('num iterations')
plt.show()

#calculate the slope of each curve as before again
slope_1 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_1[0:20]), 1)

print("Rho: 0.75 = " + str(slope_1[0]))

#we note that the slope obtained here is better than the one without using Nesterov's acceleration

#part e) training a minibatch with sgd

def train_sgd_minibatch(x_tr, y_tr, B, lam, lr, num_iter, bias=0,w_0 =None):
  c = 2
  n = np.shape(x_tr)[0]
  d = np.shape(x_tr)[1]
  w = w_0 or np.random.rand(c,d)
  w_all = []
  w_all.append(w)
  t = 0
  while t <= num_iter:
    ii = np.random.randint(n, size=B)
    x = x_tr[ii]
    y = y_tr[ii]
    w = w - lr*grad_h(w,x,y,lam, bias)
    w_all.append(w)
    t += 1
  return np.array(w_all)

#initialise some values for our params
lam = 10
B = 128
lr = 0.01
T = 100 #timesteps
w_all = train_sgd_minibatch(x_tr, y_tr,B,lam, lr, T)
loss_tr = [loss(w,x_tr, y_tr, lam)[0] for w in w_all]

#plot
t_rng = np.array(range(len(w_all)))
plt.semilogy(t_rng, loss_tr)
plt.ylabel('Training loss in log-scale')
plt.xlabel('num iterations')
plt.show()



#####double check the number of iteratuons here???

#train a minibatch w/ sgd and nesterov's algorithm
def train_sgd_minibatch_nest(x_tr, y_tr, B, rho, lam, lr, num_iter, bias=0,w_0 =None):
  c = 2
  n = np.shape(x_tr)[0]
  d = np.shape(x_tr)[1]
  u =  w_0 or np.random.rand(c,d)
  w =  w_0 or np.random.rand(c,d)
  w_all = []
  w_all.append(w)
  t = 0
  while t <= num_iter:
    j = np.random.randint(n, size=B)
    x = x_tr[j]
    y = y_tr[j]
    u_next = w - lr * grad_h(w,x,y,lam,bias)
    w = (1 + rho) * u_next - rho * u
    u = u_next
    w_all.append(w)
    t += 1
  return np.array(w_all)

#compare the 4 algorithms with rho set to 0.85
lam = 10
batch_size = 128
lr = 0.01
timesteps = 100
rho = 0.85
w_gd  = train_gd(x_tr, y_tr, lam, lr, timesteps)
w_nest_gd  = train_nesterov(x_tr, y_tr,rho,lam, lr, timesteps)
w_sgd = train_sgd_minibatch(x_tr, y_tr, batch_size,lam, lr, T)
w_nest_sgd = train_sgd_minibatch_nest(x_tr, y_tr,batch_size, rho, lam, lr, timesteps)

loss_gd = [loss(w,x_tr, y_tr, lam)[0] for w in w_gd]
loss_nest_gd = [loss(w,x_tr, y_tr, lam)[0] for w in w_nest_gd]
loss_sgd = [loss(w,x_tr, y_tr, lam)[0] for w in w_sgd]
loss_nest_sgd = [loss(w,x_tr, y_tr, lam)[0] for w in w_nest_sgd]

#plot the comparison results in log scale:)
t_rng = np.array(range(len(w_gd)))
t_rng2 = np.array(range(len(loss_sgd)))
plt.semilogy(t_rng, loss_gd)
plt.semilogy(t_rng, loss_nest_gd)
plt.semilogy(t_rng2, loss_sgd)
plt.semilogy(t_rng2, loss_nest_sgd)
plt.legend(('GD','GD with Nesterov','SGD','SGD with Nesterov') )
plt.ylabel('training loss in log-scale]')
plt.xlabel('num iterations')
plt.show()

#as we can see from our plots above, convergence of SGD is faster with Nesterov's acceleration. Convergence
#curves with and without Nesterov's acceleration look pretty different. With vanilla GD and SGD, the curves simply taper
#by decreasing steadily and then tapering off at a given number of iterations. With Nesterov's acceleration however, we see the training loss
#having spikes as the number of training iterations increases before finally tapering off as well at a given number
#of iterations - 40 iterations in our case here.

'''
NOTE: WILL INCLUDE HESSIAN CALCULATIONS IN WRITEUP!! TY!

'''